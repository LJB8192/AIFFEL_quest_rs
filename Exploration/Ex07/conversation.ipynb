{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a00e48c",
   "metadata": {},
   "source": [
    "## Package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80a6545c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a539c6f3",
   "metadata": {},
   "source": [
    "# 1. 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1848923",
   "metadata": {},
   "source": [
    "## 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bb55d92",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남.</td>\n",
       "      <td>설렜겠어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                         A  label\n",
       "0                       12시 땡!                하루가 또 가네요.      0\n",
       "1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
       "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "4                      PPL 심하네                눈살이 찌푸려지죠.      0\n",
       "...                        ...                       ...    ...\n",
       "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
       "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
       "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
       "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"ChatbotData .csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb61eb3",
   "metadata": {},
   "source": [
    "### 1) 전처리 함수: 영어에 대한 전처리 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea8db1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 전처리 함수\n",
    "def preprocess_sentence1(sentence):\n",
    "  # 입력받은 sentence를 소문자로 변경하고 양쪽 공백을 제거\n",
    "  sentence = sentence.lower().strip()  # 문장을 소문자로 변경하고 공백 제거\n",
    "\n",
    "  # 단어와 구두점(punctuation) 사이의 거리를 만듭니다.\n",
    "  # 예를 들어서 \"I am a student.\" => \"I am a student .\"와 같이\n",
    "  # student와 온점 사이에 거리를 만듭니다.\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)  # 구두점과 단어 사이에 공백 삽입\n",
    "  sentence = re.sub(r'[\" \"]+', \" \", sentence)  # 연속된 공백을 하나의 공백으로 변환\n",
    "\n",
    "  # (a-z, A-Z, \".\", \"?\", \"!\", \",\")를 제외한 모든 문자를 공백인 ' '로 대체합니다.\n",
    "  sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)  # 허용된 문자 외 모든 문자를 공백으로 변환\n",
    "  sentence = sentence.strip()  # 양쪽 공백 제거\n",
    "  return sentence\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9def1d2f",
   "metadata": {},
   "source": [
    "### 2) 전처리 함수2: 한국어에 대한 전처리 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94476826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 전처리 함수\n",
    "def preprocess_sentence2(sentence):\n",
    "  # 입력받은 sentence를 소문자로 변경하고 양쪽 공백을 제거\n",
    "    sentence = sentence.lower().strip()  # 문장을 소문자로 변경하고 공백 제거\n",
    "\n",
    "  # 단어와 구두점(punctuation) 사이의 거리를 만듭니다.\n",
    "  # 예를 들어서 \"I am a student.\" => \"I am a student .\"와 같이\n",
    "  # student와 온점 사이에 거리를 만듭니다.\n",
    "    sentence = re.sub(r'[^\\w\\W\\.\\?!,]', \"\", sentence)  # 구두점과 단어 사이에 공백 삽입\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)  # 연속된 공백을 하나의 공백으로 변환\n",
    "\n",
    "  # (a-z, A-Z, \".\", \"?\", \"!\", \",\")를 제외한 모든 문자를 공백인 ' '로 대체합니다.\n",
    "  # 허용된 문자 외 모든 문자를 공백으로 변환\n",
    "    sentence = sentence.strip()  # 양쪽 공백 제거\n",
    "    return sentence\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30ef673",
   "metadata": {},
   "source": [
    "## 문제에서 사용\n",
    "-questions과 answers에 대하여 전처리를 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2406bc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 질문과 답변의 쌍인 데이터셋을 구성하기 위한 데이터 로드 함수\n",
    "def load_conversations(df):\n",
    "    inputs, outputs = [], []\n",
    "\n",
    "    for line in df[\"A\"]:\n",
    "        inputs.append(preprocess_sentence2(line))\n",
    "    \n",
    "    for line in df['Q']: \n",
    "        outputs.append(preprocess_sentence2(line)) \n",
    "\n",
    "    return inputs, outputs\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80cc5576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수 : 11823\n",
      "전체 샘플 수 : 11823\n",
      "전처리 후의 23번째 질문 샘플: 가장 확실한 시간은 오늘이에요. 어제와 내일을 놓고 고민하느라 시간을 낭비하지 마세요.\n",
      "전처리 후의 23번째 답변 샘플: 가장 확실한 건 뭘까?\n"
     ]
    }
   ],
   "source": [
    "questions, answers =load_conversations(df)\n",
    "print('전체 샘플 수 :', len(questions))\n",
    "print('전체 샘플 수 :', len(answers))\n",
    "print('전처리 후의 23번째 질문 샘플: {}'.format(questions[22]))\n",
    "print('전처리 후의 23번째 답변 샘플: {}'.format(answers[22]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faec2df7",
   "metadata": {},
   "source": [
    "# 2. SubwordTextEncoder 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c3dc78",
   "metadata": {},
   "source": [
    "## tokenizer 사용하기: 단어장 만들기\n",
    "단어들에 index를 부여하기 위하여 tokenizer라는 단어장을 미리 만드는 과정이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75e11cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5607b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "# 문장의 시작과 끝에 해당하는 token을 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80c17eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKEN의 번호 : [8170]\n",
      "END_TOKEN의 번호 : [8171]\n"
     ]
    }
   ],
   "source": [
    "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])\n",
    "# 시작과 끝 token 번호"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbba06db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8172\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = tokenizer.vocab_size + 2 # 단어장 크기 더하기 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "749862d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 21번째 질문 샘플: [2353, 7510, 5, 6273, 94, 7960]\n",
      "정수 인코딩 후의 21번째 답변 샘플: [5758, 607, 2490, 4160]\n"
     ]
    }
   ],
   "source": [
    "print('정수 인코딩 후의 21번째 질문 샘플: {}'.format(tokenizer.encode(questions[21])))\n",
    "print('정수 인코딩 후의 21번째 답변 샘플: {}'.format(tokenizer.encode(answers[21])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d272123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문의 최대 길이 : 76\n",
      "질문의 평균 길이 : 15.0151399813922\n",
      "답변의 최대 길이 : 76\n",
      "답변의 평균 길이 : 12.879049310665652\n"
     ]
    }
   ],
   "source": [
    "print('질문의 최대 길이 :',max(len(question) for question in questions))\n",
    "print('질문의 평균 길이 :',sum(map(len, questions))/len(questions))\n",
    "print('답변의 최대 길이 :',max(len(question) for question in questions))\n",
    "print('답변의 평균 길이 :',sum(map(len, answers))/len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8813417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 20\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b13f4c",
   "metadata": {},
   "source": [
    "## 정수 Indexing과 padding을 함수화\n",
    "-token들에 각각의 index를 부여하게 된다.  \n",
    "-filtering을 통하여 특정 크기 이하의 문장들을 제거한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "948a94bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    # 최대 길이 20 이하인 경우에만 데이터셋으로 허용\n",
    "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "      tokenized_inputs.append(sentence1)\n",
    "      tokenized_outputs.append(sentence2)\n",
    "  \n",
    "  # 최대 길이 20으로 모든 데이터셋을 패딩\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  \n",
    "  return tokenized_inputs, tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "853270e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 8172\n",
      "필터링 후의 질문 샘플 개수: 11791\n",
      "필터링 후의 답변 샘플 개수: 11791\n"
     ]
    }
   ],
   "source": [
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "print('필터링 후의 질문 샘플 개수: {}'.format(len(questions)))\n",
    "print('필터링 후의 답변 샘플 개수: {}'.format(len(answers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f540577a",
   "metadata": {},
   "source": [
    "# 3. 모델 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b729b77a",
   "metadata": {},
   "source": [
    "### 1)교사 학습\n",
    "\n",
    "언어 모델의 **교사 강요(Teacher Forcing)**는 자연어 처리(NLP) 모델을 훈련할 때 사용하는 중요한 기법 중 하나입니다. 주로 시퀀스 생성 모델(Sequence Generation Models), 예를 들어 **RNN(Recurrent Neural Network)**이나 Transformer 모델을 훈련할 때 사용됩니다.\n",
    "\n",
    "기본 개념\n",
    "\n",
    "교사 강요는 모델이 훈련 과정에서 이전에 예측한 값을 기반으로 다음 토큰을 생성하는 대신, **정답 데이터(ground truth)**에서 제공된 실제 값을 사용하여 다음 토큰을 예측하도록 하는 방법입니다. 이를 통해 모델이 빠르게 학습하고, 예측 오류가 누적되는 것을 방지할 수 있습니다.\n",
    "\n",
    "작동 방식\n",
    "\n",
    "1. 입력: 모델은 주어진 시퀀스의 첫 번째 단어를 입력받고, 그에 따라 다음 단어를 예측합니다.\n",
    "\n",
    "\n",
    "2. 교사 강요: 모델이 예측한 단어가 아니라, 실제 정답인 단어를 사용하여 다음 단어를 예측하게 만듭니다.\n",
    "\n",
    "예: \"The cat is\"라는 문장이 있을 때, 첫 번째 단어 \"The\"에 기반해 모델이 \"dog\"을 예측했더라도, 실제 정답인 \"cat\"을 다음 입력으로 사용하여 \"is\"를 예측하게 합니다.\n",
    "\n",
    "\n",
    "\n",
    "3. 이 과정을 반복하며 전체 시퀀스를 학습합니다.\n",
    "\n",
    "\n",
    "\n",
    "장점\n",
    "\n",
    "빠른 수렴: 교사 강요는 모델이 학습하는 동안 매번 올바른 입력을 제공받기 때문에, 모델이 더 빨리 수렴할 수 있습니다.\n",
    "\n",
    "오차 누적 방지: 모델이 학습 초기에 발생하는 작은 예측 오류가 점점 커지는 문제를 방지합니다.\n",
    "\n",
    "\n",
    "단점\n",
    "\n",
    "차이점 학습 문제: 실제 예측 시에는 모델이 이전에 예측한 값들을 기반으로 시퀀스를 생성해야 하는데, 교사 강요는 항상 정답을 기준으로 다음 값을 예측하게 합니다. 이 때문에 훈련 단계와 추론 단계 간의 차이로 인해 모델이 실제 예측 시 성능이 저하될 수 있습니다.\n",
    "\n",
    "이를 **노출 편향(Exposure Bias)**이라고 합니다.\n",
    "\n",
    "\n",
    "\n",
    "대안 방법\n",
    "\n",
    "이 문제를 해결하기 위해 일부 연구에서는 교사 강요 비율을 점진적으로 줄여가는 방법이나, 학습 후반부에는 모델의 예측 값을 사용해 훈련하는 방법 등을 제안하기도 합니다. 예를 들어, Scheduled Sampling은 정답과 모델의 예측값을 일정 비율로 섞어 학습하는 기법입니다.\n",
    "\n",
    "요약\n",
    "\n",
    "교사 강요는 모델의 학습 효율을 높이기 위해 정답 데이터를 사용하여 다음 단어를 예측하게 하는 방법으로, 훈련과 추론 간의 차이점을 고려해야 하지만, 기본적으로 효과적인 학습 기법입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "126a8180",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
    "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7ff41f",
   "metadata": {},
   "source": [
    "### 2) Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c73d0c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포지셔널 인코딩 레이어\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, position, d_model):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "  def get_angles(self, position, i, d_model):\n",
    "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    return position * angles\n",
    "\n",
    "  def positional_encoding(self, position, d_model):\n",
    "    # 각도 배열 생성\n",
    "    angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "\n",
    "    # 배열의 짝수 인덱스에는 sin 함수 적용\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "    # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    # sin과 cosine이 교차되도록 재배열\n",
    "    pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "    pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
    "    pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f0ff7a",
   "metadata": {},
   "source": [
    "### 3) Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fab92da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일드 닷 프로덕트 어텐션 함수\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # 가중치를 정규화\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # 패딩에 마스크 추가\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "\n",
    "  # softmax적용\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
    "  output = tf.matmul(attention_weights, value)\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e1440ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # Q, K, V에 각각 Dense를 적용합니다\n",
    "    query = self.query_dense(query)   # 쿼리에 Dense 적용\n",
    "    key = self.key_dense(key)         # 키에 Dense 적용\n",
    "    value = self.value_dense(value)   # 값에 Dense 적용\n",
    "\n",
    "    # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
    "    query = self.split_heads(query, batch_size)  # 쿼리를 여러 헤드로 나눔\n",
    "    key = self.split_heads(key, batch_size)      # 키를 여러 헤드로 나눔\n",
    "    value = self.split_heads(value, batch_size)  # 값을 여러 헤드로 나눔\n",
    "\n",
    "    # 스케일드 닷 프로덕트 어텐션 함수\n",
    "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "    # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
    "    outputs = self.dense(concat_attention)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34796728",
   "metadata": {},
   "source": [
    "### 4) Padding Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ca66501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  # (batch_size, 1, 1, sequence length)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e4907c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "  seq_len = tf.shape(x)[1]\n",
    "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "  padding_mask = create_padding_mask(x)\n",
    "  return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c0b472",
   "metadata": {},
   "source": [
    "### 5) Encoder의 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00301e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "  # 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "  attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 2개의 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47d2f817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "  # 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 임베딩 레이어\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # 포지셔널 인코딩\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # num_layers만큼 쌓아올린 인코더의 층.\n",
    "  for i in range(num_layers):\n",
    "    outputs = encoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name=\"encoder_layer_{}\".format(i),\n",
    "    )([outputs, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba2f84b",
   "metadata": {},
   "source": [
    "### 6) Decoder의 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e6b2021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "\n",
    "  # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
    "  attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # 마스크드 멀티 헤드 어텐션의 결과는\n",
    "  # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "  attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "  # 세 번째 서브 레이어 : 2개의 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c3081dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "  # 패딩 마스크\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "  \n",
    "  # 임베딩 레이어\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # 포지셔널 인코딩\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  for i in range(num_layers):\n",
    "    outputs = decoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name='decoder_layer_{}'.format(i),\n",
    "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f733a1",
   "metadata": {},
   "source": [
    "### 7) Transformer의 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc6091fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "  # 인코더에서 패딩을 위한 마스크\n",
    "  enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "\n",
    "  # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
    "  # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
    "  look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask,\n",
    "      output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "  # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
    "  # 디코더에서 패딩을 위한 마스크\n",
    "  dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "  # 인코더\n",
    "  enc_outputs = encoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "  # 디코더\n",
    "  dec_outputs = decoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "  # 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ddbdf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 256)    3146240     inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 256)    3673600     dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8172)   2100204     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,920,044\n",
      "Trainable params: 8,920,044\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 256 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0b283f",
   "metadata": {},
   "source": [
    "### 8) 손실 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb2203b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  \n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "  loss = tf.multiply(loss, mask)\n",
    "\n",
    "  return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e38a08",
   "metadata": {},
   "source": [
    "### 9) 커스텀 된 학습률(Learning Rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2440b2",
   "metadata": {},
   "source": [
    "이 코드는 학습률 스케줄러를 정의한 것으로, 학습 과정 중 학습률(learning rate)을 동적으로 조정하기 위한 클래스를 구현한 것입니다. 이 스케줄러는 주로 Transformer 모델에서 사용되는 Custom Learning Rate Schedule을 구현한 형태로, 일정 단계 동안 학습률을 서서히 증가시키고 그 이후에는 감소시키는 패턴을 따릅니다.\n",
    "\n",
    "코드 설명\n",
    "1. CustomSchedule 클래스:\n",
    "CustomSchedule 클래스는 tf.keras.optimizers.schedules.LearningRateSchedule을 상속받아, 학습률 스케줄을 정의하는 클래스입니다. 이 클래스는 학습 단계에 따라 학습률을 조절합니다.\n",
    "__init__ 메서드는 초기화를 담당하며, d_model과 warmup_steps를 인자로 받습니다. d_model은 모델의 차원, warmup_steps는 학습률을 서서히 증가시키는 단계 수를 설정합니다.\n",
    "2. call 메서드:\n",
    "이 메서드는 매 학습 스텝에서 호출되며, 해당 학습 스텝에 맞는 학습률을 계산하여 반환합니다.\n",
    "인자로 받는 step은 현재 학습 단계(스텝)입니다.\n",
    "3. 학습률 계산 로직:\n",
    "학습률은 다음 두 가지 요인을 결합하여 계산됩니다:\n",
    "arg1 = tf.math.rsqrt(step): 현재 스텝의 역 제곱근을 계산합니다. 학습이 진행될수록 이 값은 감소합니다.\n",
    "arg2 = step * (self.warmup_steps**-1.5): 현재 스텝을 warmup_steps의 -1.5승과 곱합니다. 초기에는 이 값이 작아져서 학습률이 서서히 증가하도록 합니다.\n",
    "최종 학습률은 arg1과 arg2 중 더 작은 값에 tf.math.rsqrt(self.d_model)를 곱하여 결정됩니다. 즉, 학습 초기에는 학습률이 증가(warmup)하고, 특정 스텝 이후에는 감소하는 패턴을 따릅니다.\n",
    "4. 학습률의 동작 방식:\n",
    "Warmup phase: step이 작을 때는 arg2가 지배적이므로 학습률이 선형적으로 증가합니다. 이로 인해 초기 학습에서의 불안정성을 줄일 수 있습니다.\n",
    "Decay phase: step이 커질수록 arg1이 지배적이 되어 학습률이 점차 감소하게 됩니다. 이로써 학습이 안정되면서 학습률을 줄여 모델이 더 정교하게 학습되도록 합니다.\n",
    "요약:\n",
    "이 스케줄러는 학습 초기에는 학습률을 선형적으로 증가시키고(이 과정이 warmup), 이후 학습률을 감소시키는 방식으로 동작합니다.\n",
    "주로 Transformer와 같은 모델에서 학습 안정성 및 성능을 높이기 위해 사용됩니다.\n",
    "스케줄링을 통해 모델이 초기 학습률로 인해 급격하게 최적화되지 않도록 하고, 이후 학습이 안정되면 학습률을 줄여 더 세밀한 학습을 할 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc88edc",
   "metadata": {},
   "source": [
    "\n",
    "Warmup은 딥러닝에서 **학습률(learning rate)**을 초기 학습 단계에서 점진적으로 증가시키는 기법을 말합니다. 이를 통해 모델이 학습 초기의 불안정성을 피하고, 더 안정적으로 최적화될 수 있도록 도와줍니다. Warmup은 주로 학습이 불안정한 복잡한 모델이나, 학습 초기에 큰 학습률을 사용할 경우 성능이 나빠지는 경우에 유용합니다.\n",
    "\n",
    "Warmup의 필요성\n",
    "딥러닝 모델을 학습할 때, 학습률은 모델의 성능과 수렴 속도에 큰 영향을 미칩니다. 학습률이 너무 크면 최적화 과정에서 과도하게 큰 업데이트가 이루어져서 모델이 불안정해질 수 있고, 학습률이 너무 작으면 학습이 느려지고 수렴 속도가 늦어질 수 있습니다. 특히 초기 학습 단계에서는 모델이 아직 잘 정돈되지 않은 상태이므로, 큰 학습률로 시작하면 모델이 최적화에서 벗어나거나 성능이 나빠질 수 있습니다.\n",
    "\n",
    "Warmup의 기본 아이디어\n",
    "Warmup은 학습 초기에는 낮은 학습률로 학습을 시작하고, 이후 일정한 단계까지 학습률을 점진적으로 증가시키는 방식입니다. 이를 통해 모델이 학습 초기에 안정적으로 최적화되도록 하고, 점차 학습률을 증가시켜 더 빠른 학습이 가능해지도록 합니다.\n",
    "\n",
    "Warmup의 적용 방식:\n",
    "초기 학습률: 학습이 시작되면, 매우 낮은 학습률로 모델을 학습시킵니다.\n",
    "점진적 증가: 학습률을 일정한 단계(warmup steps) 동안 점진적으로 증가시킵니다.\n",
    "최대 학습률 도달: warmup 단계가 끝나면, 학습률은 설정된 최대 학습률에 도달하게 됩니다.\n",
    "학습률 조정: 이후에는 학습률이 고정되거나, **학습률 감소(Learning Rate Decay)**를 적용하여 학습률을 줄이는 방식으로 학습을 계속합니다.\n",
    "Warmup의 예\n",
    "Transformer와 같은 대규모 모델에서 warmup은 자주 사용됩니다. 특히 초기 단계에서 학습률을 천천히 증가시키고, 이후 학습률을 줄이는 방식이 안정적인 학습을 가능하게 합니다. 이를 통해 모델이 수렴 과정에서 큰 폭으로 요동치지 않도록 돕습니다.\n",
    "\n",
    "Warmup의 장점\n",
    "학습 초기의 불안정성 완화: 초기 학습에서 너무 큰 학습률을 사용하는 경우 모델이 불안정해지거나, 최적화가 비효율적으로 이루어질 수 있습니다. Warmup은 이 문제를 완화해줍니다.\n",
    "더 안정적이고 효과적인 최적화: Warmup을 통해 모델이 학습 초기에 작은 학습률로 안정적인 학습을 하다가, 충분히 안정화된 후에 학습률을 크게 하여 빠르게 학습할 수 있습니다.\n",
    "복잡한 모델에 유리: 대규모 모델이나 복잡한 데이터셋을 다룰 때, Warmup은 모델이 더 안정적으로 학습할 수 있도록 도와줍니다.\n",
    "Warmup이 자주 사용되는 사례\n",
    "Transformer 모델: 자연어 처리(NLP)에서 Transformer 구조는 초기 학습에서 Warmup을 사용하는데, 이는 매우 큰 모델의 복잡성을 다루기 위함입니다.\n",
    "대규모 신경망: 매우 깊거나 복잡한 신경망에서 학습 초기에 학습률을 점진적으로 증가시키는 것이 학습의 안정성과 성능에 유리합니다.\n",
    "요약\n",
    "Warmup은 학습 초기에는 낮은 학습률로 시작하여 점진적으로 학습률을 증가시키는 방법으로, 초기 학습 불안정성을 줄이고 최적화 성능을 높일 수 있습니다. 특히 복잡한 모델이나 대규모 데이터셋을 다룰 때 유용하며, Transformer와 같은 모델에서 자주 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1020e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccde8098",
   "metadata": {},
   "source": [
    "### 10) 모델 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6380637",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "135c7aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "185/185 [==============================] - 13s 37ms/step - loss: 2.8997 - accuracy: 0.0401\n",
      "Epoch 2/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 2.4997 - accuracy: 0.0526\n",
      "Epoch 3/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 2.2782 - accuracy: 0.0548\n",
      "Epoch 4/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 2.1598 - accuracy: 0.0594\n",
      "Epoch 5/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 2.0606 - accuracy: 0.0659\n",
      "Epoch 6/100\n",
      "185/185 [==============================] - 7s 38ms/step - loss: 1.9559 - accuracy: 0.0724\n",
      "Epoch 7/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 1.8440 - accuracy: 0.0792\n",
      "Epoch 8/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 1.7218 - accuracy: 0.0875\n",
      "Epoch 9/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 1.5870 - accuracy: 0.0983\n",
      "Epoch 10/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 1.4357 - accuracy: 0.1118\n",
      "Epoch 11/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 1.2736 - accuracy: 0.1284\n",
      "Epoch 12/100\n",
      "185/185 [==============================] - 7s 36ms/step - loss: 1.1035 - accuracy: 0.1477\n",
      "Epoch 13/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.9339 - accuracy: 0.1696\n",
      "Epoch 14/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.7748 - accuracy: 0.1921\n",
      "Epoch 15/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.6295 - accuracy: 0.2145\n",
      "Epoch 16/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.5061 - accuracy: 0.2348\n",
      "Epoch 17/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.4112 - accuracy: 0.2520\n",
      "Epoch 18/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.3447 - accuracy: 0.2648\n",
      "Epoch 19/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.3061 - accuracy: 0.2722\n",
      "Epoch 20/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.2828 - accuracy: 0.2763\n",
      "Epoch 21/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.2705 - accuracy: 0.2796\n",
      "Epoch 22/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.2625 - accuracy: 0.2819\n",
      "Epoch 23/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.2502 - accuracy: 0.2842\n",
      "Epoch 24/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.2287 - accuracy: 0.2896\n",
      "Epoch 25/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.2148 - accuracy: 0.2933\n",
      "Epoch 26/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.2027 - accuracy: 0.2961\n",
      "Epoch 27/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.1922 - accuracy: 0.2989\n",
      "Epoch 28/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.1835 - accuracy: 0.3007\n",
      "Epoch 29/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.1767 - accuracy: 0.3028\n",
      "Epoch 30/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.1689 - accuracy: 0.3039\n",
      "Epoch 31/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.1630 - accuracy: 0.3050\n",
      "Epoch 32/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.1575 - accuracy: 0.3064\n",
      "Epoch 33/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.1521 - accuracy: 0.3076\n",
      "Epoch 34/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.1466 - accuracy: 0.3082\n",
      "Epoch 35/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.1426 - accuracy: 0.3089\n",
      "Epoch 36/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.1383 - accuracy: 0.3097\n",
      "Epoch 37/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.1341 - accuracy: 0.3102\n",
      "Epoch 38/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.1301 - accuracy: 0.3109\n",
      "Epoch 39/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.1273 - accuracy: 0.3114\n",
      "Epoch 40/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.1240 - accuracy: 0.3117\n",
      "Epoch 41/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.1206 - accuracy: 0.3119\n",
      "Epoch 42/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.1180 - accuracy: 0.3119\n",
      "Epoch 43/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.1142 - accuracy: 0.3124\n",
      "Epoch 44/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.1116 - accuracy: 0.3126\n",
      "Epoch 45/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.1100 - accuracy: 0.3130\n",
      "Epoch 46/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.1071 - accuracy: 0.3131\n",
      "Epoch 47/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.1047 - accuracy: 0.3134\n",
      "Epoch 48/100\n",
      "185/185 [==============================] - 7s 36ms/step - loss: 0.1021 - accuracy: 0.3135\n",
      "Epoch 49/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.1002 - accuracy: 0.3138\n",
      "Epoch 50/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0978 - accuracy: 0.3141\n",
      "Epoch 51/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0965 - accuracy: 0.3138\n",
      "Epoch 52/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0950 - accuracy: 0.3141\n",
      "Epoch 53/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0935 - accuracy: 0.3142\n",
      "Epoch 54/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0911 - accuracy: 0.3142\n",
      "Epoch 55/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0899 - accuracy: 0.3145\n",
      "Epoch 56/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0878 - accuracy: 0.3146\n",
      "Epoch 57/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0873 - accuracy: 0.3145\n",
      "Epoch 58/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0864 - accuracy: 0.3145\n",
      "Epoch 59/100\n",
      "185/185 [==============================] - 7s 36ms/step - loss: 0.0846 - accuracy: 0.3147\n",
      "Epoch 60/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0842 - accuracy: 0.3147\n",
      "Epoch 61/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0829 - accuracy: 0.3147\n",
      "Epoch 62/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0809 - accuracy: 0.3149\n",
      "Epoch 63/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0809 - accuracy: 0.3144\n",
      "Epoch 64/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0790 - accuracy: 0.3149\n",
      "Epoch 65/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0793 - accuracy: 0.3152\n",
      "Epoch 66/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0783 - accuracy: 0.3150\n",
      "Epoch 67/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0774 - accuracy: 0.3149\n",
      "Epoch 68/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0764 - accuracy: 0.3153\n",
      "Epoch 69/100\n",
      "185/185 [==============================] - 7s 38ms/step - loss: 0.0760 - accuracy: 0.3151\n",
      "Epoch 70/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0751 - accuracy: 0.3152\n",
      "Epoch 71/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0744 - accuracy: 0.3152\n",
      "Epoch 72/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0742 - accuracy: 0.3153\n",
      "Epoch 73/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0729 - accuracy: 0.3152\n",
      "Epoch 74/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0730 - accuracy: 0.3152\n",
      "Epoch 75/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0721 - accuracy: 0.3154\n",
      "Epoch 76/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0720 - accuracy: 0.3153\n",
      "Epoch 77/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0716 - accuracy: 0.3154\n",
      "Epoch 78/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0712 - accuracy: 0.3154\n",
      "Epoch 79/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0701 - accuracy: 0.3157\n",
      "Epoch 80/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0691 - accuracy: 0.3157\n",
      "Epoch 81/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0688 - accuracy: 0.3156\n",
      "Epoch 82/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0687 - accuracy: 0.3156\n",
      "Epoch 83/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0687 - accuracy: 0.3153\n",
      "Epoch 84/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0675 - accuracy: 0.3156\n",
      "Epoch 85/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0673 - accuracy: 0.3155\n",
      "Epoch 86/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0677 - accuracy: 0.3154\n",
      "Epoch 87/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0667 - accuracy: 0.3157\n",
      "Epoch 88/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0666 - accuracy: 0.3155\n",
      "Epoch 89/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0661 - accuracy: 0.3155\n",
      "Epoch 90/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0659 - accuracy: 0.3154\n",
      "Epoch 91/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0658 - accuracy: 0.3157\n",
      "Epoch 92/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0652 - accuracy: 0.3156\n",
      "Epoch 93/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0644 - accuracy: 0.3158\n",
      "Epoch 94/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0638 - accuracy: 0.3158\n",
      "Epoch 95/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0647 - accuracy: 0.3156\n",
      "Epoch 96/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0640 - accuracy: 0.3154\n",
      "Epoch 97/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0639 - accuracy: 0.3158\n",
      "Epoch 98/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0638 - accuracy: 0.3157\n",
      "Epoch 99/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0626 - accuracy: 0.3159\n",
      "Epoch 100/100\n",
      "185/185 [==============================] - 7s 37ms/step - loss: 0.0627 - accuracy: 0.3158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7e6da728d550>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3454c7",
   "metadata": {},
   "source": [
    "# 4. 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2c4901ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference(sentence):\n",
    "  sentence = preprocess_sentence2(sentence)\n",
    "\n",
    "  # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
    "  # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
    "  sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
    "  # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
    "  output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # 디코더의 인퍼런스 단계\n",
    "  for i in range(MAX_LENGTH):\n",
    "    # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
    "    predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "    predictions = predictions[:, -1:, :]\n",
    "\n",
    "    # 현재 예측한 단어의 정수\n",
    "    predicted_id = tf.cast(tf.argma x(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "\n",
    "    # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
    "    # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
    "    output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output_sequence, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c1117cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(sentence):\n",
    "  # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
    "  prediction = decoder_inference(sentence)\n",
    "\n",
    "  # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
    "  predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "  print('입력 : {}'.format(sentence))\n",
    "  print('출력 : {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b391c918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 1 더하기 1은?\n",
      "출력 : 짝남이 내가 좋아하는 거 알아버림.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'짝남이 내가 좋아하는 거 알아버림.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374fdc41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
